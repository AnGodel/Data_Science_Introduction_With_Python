{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 27 - Clasificación: Regresión Logística\n",
    "\n",
    "\n",
    "* En este Notebook vamos a ver como solucionar problemas de ***Clasificación con la Regresión Logística*** y como se implementaría con  la técnica del gradiente descendente.\n",
    "\n",
    "\n",
    "* Vamos a tratar los siguientes puntos:\n",
    "<span></span><br>\n",
    "    1. [Clasificación](#M1)\n",
    "<span></span><br>\n",
    "    2. [Regresión Logística](#M2)\n",
    "<span></span><br>\n",
    "    3. [Gradiente Descendente: Regresión Logística](#M3)\n",
    "<span></span><br>\n",
    "    4. [Pseudocódigo del Gradiente Descendente para la Regresión Logística](#M4)\n",
    "<span></span><br>\n",
    "    5. [Implementación: Regresión Logística](#M5)\n",
    "<span></span><br>\n",
    "    6. [Ejemplo de Regresión Logística con Iris](#M6)\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "# <a name=\"M1\">1. Clasificación</a>\n",
    "\n",
    "\n",
    "* ***La Clasificación es un método utilizado para predecir un resultado categórico*** (un valor discreto y finito) ***dadas unas características o atributos que describen al elemento a clasificar***.\n",
    "\n",
    "\n",
    "* En la clasificación los ***datos de entrada ‘X’ son arbitrarios y la salida ‘Y’ es un conjunto finito*** y generalmente pequeño de N elementos Y={1,2,…,N}\n",
    "\n",
    "\n",
    "* En función del número de categorías a clasificar tenemos:\n",
    "<span></span><br><br>\n",
    "    - ***Clasificación Binaria***: Proceso que consiste en clasificar los elementos en dos categorías Y = {0, 1}\n",
    "<span></span><br><br>\n",
    "    - ***Clasificación Múltiple***: Proceso que consiste en clasificar los elementos en mas de dos categorías Y = {0, 1, …, N}\n",
    "    \n",
    "    \n",
    "#### - Ejemplos: Clasificación Binaria\n",
    "\n",
    "- Email: {Spam, No Spam}\n",
    "- Tumor: {Maligno, Benigno}\n",
    "- Transacción Fraudulenta: {Si, No}\n",
    "\n",
    "<img src=\"./imgs/26_01_cls.png\" style=\"width: 200px;\"/>\n",
    "\n",
    "#### - Ejemplos: Clasificación Múltiple\n",
    "\n",
    "- Animales: {Mamíferos, Aves, Reptiles, Peces}\n",
    "- Pelo: {Moreno, Rubio, Pelirrojo}\n",
    "- Raza: {Blanco, Negro, Amarillo}\n",
    "\n",
    "<img src=\"./imgs/26_02_cls.png\" style=\"width: 200px;\"/>\n",
    "\n",
    "\n",
    "* Los problemas de clasificación se puede resolver de una forma similar a la regresión. El objetivo es del de ***encontrar una “línea” (Decision Boundary) que separe las dos clases a clasificar***:\n",
    "\n",
    "\n",
    "* A esta \"linea\" se le conoce como \"Decision Boundary\" (Decisión de Frontera)\n",
    "\n",
    "\n",
    "<img src=\"./imgs/26_03_cls.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "* La decisión de frontera (o Decision Boundary) nos la va a dar una **\"Función de Decisión\"** del siguiente estilo: \n",
    "\n",
    "<span style=\"font-size:16px\">$$Z = \\beta_0 + \\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2$$</span>\n",
    " \n",
    "\n",
    "* Por lo que el ***objetivo de la Regresión Logística es encontrar los parámetros $\\beta_n$ que minimizen el error cometido por la función de decisión*** a la hora de clasificar.\n",
    "    \n",
    "    \n",
    "* Dada la imagen anterior, el objetivo es clasificar los elementos a partir de la función de decisión:\n",
    "<span></span><br><br>\n",
    "<span style=\"font-size:16px; color:green\">$$Verde \\rightarrow \\beta_0 + \\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2 < 0$$</span>\n",
    "<span></span><br><br>\n",
    "<span style=\"font-size:16px; color:red\">$$Rojo \\rightarrow \\beta_0 + \\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2 \\geq 0$$</span>\n",
    "\n",
    "  \n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "# <a name=\"M2\">2. Regresión Logística</a>\n",
    "\n",
    "\n",
    "* En la clasificación interesa no solo saber a que clase pertenece, si no saber también ***la probabilidad de pertenencia a una clase***.\n",
    "\n",
    "\n",
    "* Para obtener la “probabilidad” o un valor entre 0 y 1 utilizamos la ***función sigmoidal que transforma un cualquier valor continuo en un valor entre 0 y 1*** (de ahí el nombre de Regresión Logística).\n",
    "\n",
    "<img src=\"./imgs/26_04_cls.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "* Por tanto para obtener la probabilidad de que un elemento pertenezca a una clase debemos de aplicar la función sigmoidal:\n",
    "<span></span><br><br>\n",
    "<span style=\"font-size:20px\">$$g(Z) = \\frac{1}{1 + e^{-Z}} \\rightarrow Siendo: Z= \\beta_0 + \\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2$$</span>\n",
    "    \n",
    "\n",
    "* La probabilidad de que el elemento pertenezca a la clase 1 es:\n",
    "<span></span><br><br>\n",
    "<span style=\"font-size:16px\">$$P(Y=1|X_1,X_2) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2)}}$$</span>\n",
    "\n",
    "\n",
    "* La probabilidad de que el elemento pertenezca a la clase 0 es:\n",
    "<span></span><br><br>\n",
    "<span style=\"font-size:16px\">$$P(Y=0|X_1,X_2) = 1 - \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2)}}$$</span>\n",
    "\n",
    "    \n",
    "    \n",
    "#### Ejemplo: \n",
    "\n",
    "\n",
    "<img src=\"./imgs/26_05_cls.png\" style=\"width: 900px;\"/>\n",
    "\n",
    "\n",
    "* Viendo los resultados sobre una sigmoide, tenemos lo siguiente:\n",
    "\n",
    "\n",
    "<img src=\"./imgs/26_06_cls.png\" style=\"width: 900px;\"/>\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "# <a name=\"M3\">3. Gradiente Descendente: Regresión Logística</a>\n",
    "\n",
    "\n",
    "* Con el gradiente descendente podemos encontrar los parámetros $\\beta_0$, $\\beta_1$, $\\beta_2$ (para el caso de clasificación binaria) que mejor ajusten la ***función de decisión*** entre las dos clases:\n",
    "\n",
    "<span style=\"font-size:20px\">$$Z = \\beta_0 + \\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2$$</span>\n",
    "\n",
    "\n",
    "* Para obtener estos parámetros necesitamos ***definir una función de perdida*** que nos diga cual es nuestro ***error para los datos de entrenamiento*** dados unos valores de $\\beta_0$, $\\beta_1$, $\\beta_2$.\n",
    "\n",
    "\n",
    "* El error para los problemas de clasificación lo podemos definir como:\n",
    "\n",
    "<span style=\"font-size:16px\">$$error(x^{(i)},y^{(i)},h(x^{(i)})) \\left\\{\\begin{matrix}\n",
    "1\\;si\\;h(x^{(i)})\\neq y^{(i)}\\\\ \n",
    "0\\;si\\;h(x^{(i)})=y^{(i)}\n",
    "\\end{matrix}\\right.$$</span>\n",
    "\n",
    "\n",
    "* Siendo $h(x^{(i)})$ un clasificador binario que devuelve dos valores $[0,1]$\n",
    "\n",
    "\n",
    "* Pero dado que la ***Regresión Logística*** nos ***devuelve una probabilidad de pertenencia a una clase entre 0 y 1, no podemos tomar ese error para la función de coste*** ya que es un error para un problema discriminante y no para un problema probabilístico.\n",
    "\n",
    "\n",
    "* Definimos el ***error en la regresión logística*** para un elemento en concreto como:\n",
    "\n",
    "<span style=\"font-size:16px\">$$error(x^{(i)},y^{(i)},h(x^{(i)})) = \\left\\{\\begin{matrix}\n",
    "-log(h(x^{(i)}))\\;si\\;y=1\\\\ \n",
    "-log(1 - h(x^{(i)}))\\;si\\;y=0\n",
    "\\end{matrix}\\right.$$</span>\n",
    "\n",
    "\n",
    "* Pudiendo simplificar este ***Error*** como:\n",
    "\n",
    "<span style=\"font-size:16px\">$$error(x^{(i)},y^{(i)},h(x^{(i)})) = -y^{(i)} \\cdot log(h(x^{(i)})) - (1 - y^{(i)}) \\cdot log(1 - h(x^{(i)}))$$</span>\n",
    "    \n",
    "    \n",
    "* Siendo:\n",
    "\n",
    "<span style=\"font-size:16px\">$$h(x^{(i)}) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2)}}$$</span>\n",
    "\n",
    "\n",
    "* Definimos la ***Función de Perdida*** como:\n",
    "\n",
    "<span style=\"font-size:16px\">$$función\\;perdida = E(parámetros) = \\frac{1}{N} \\sum_{i=1}^{N} error(h(x^{(i)}), y^{(i)})$$</span>\n",
    "    \n",
    "    \n",
    "* O lo que es lo mismo\n",
    "\n",
    "<span style=\"font-size:16px\">$$función\\;perdida = E(parámetros) = \\frac{-1}{N} \\sum_{i=1}^{N} y^{(i)} \\cdot log(h(x^{(i)})) + (1 - y^{(i)}) \\cdot log(1 - h(x^{(i)}))$$</span>\n",
    "\n",
    "\n",
    "* Por tanto, el objetivo es el de encontrar los parámetros $\\beta_n$ que minimizen la función de perdida:\n",
    "\n",
    "<span style=\"font-size:16px\">$$\\underset{\\beta_0, \\beta_1, \\beta_2}{min} E(\\beta_0, \\beta_1, \\beta_2)  = \\frac{-1}{N} \\sum_{i=1}^{N} y^{(i)} \\cdot log(\\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 \\cdot X^{(i)}_1 + \\beta_2 \\cdot X^{(i)}_2)}}) + (1 - y^{(i)}) \\cdot log(1 - \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 \\cdot X^{(i)}_1 + \\beta_2 \\cdot X^{(i)}_2)}})$$</span>\n",
    "\n",
    "\n",
    "* La técnica del gradiente descendente consiste en ***derivar de forma parcial la función de perdida para cada uno de los parámetros $\\beta_n$ y moverse una determinada cantidad (α) en el sentido contrario a la pendiente de la derivada***, con el objetivo de encontrar los parámetros $\\beta_n$ que minimicen la función de perdida:\n",
    "<span></span><br><br>   \n",
    "<span style=\"font-size:16px\">$$\\beta^{new}_{j} := \\beta^{old}_{j} - \\alpha  \\frac{\\partial }{\\partial \\beta_j}E(\\beta_0,...,\\beta_j)$$</span>\n",
    "    \n",
    "    \n",
    "* Para ello ***necesitamos tener un modelo (o hipótesis) inicial*** que podemos generar de diferentes maneras (por ejemplo de forma aleatoria) y ***tener unos valores iniciales de $\\beta_n$ que vayamos ajustando en función del error cometido***.\n",
    "\n",
    "\n",
    "* La ***forma de ajustar estos valores $\\beta_n$ seria derivando de manera parcial la función de perdida respecto a cada $\\beta_n$***, quedando esta actualización de la siguiente forma (NOTA: $\\beta_0$ se calcula de manera diferente al resto de $\\beta_n$):\n",
    "<span></span><br><br>   \n",
    "<span style=\"font-size:16px\">$$\\beta_{n}^{new} := \\beta_{n}^{old} - \\alpha  \\frac{1}{N}\\sum_{i=1}^{N} ((\\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 \\cdot X^{(i)}_1 + \\beta_2 \\cdot X^{(i)}_2)}}) - y^{(i)}) \\cdot x_{n}^{(i)}$$</span>\n",
    "<span></span><br><br> \n",
    "<span style=\"font-size:16px\">$$\\beta_{0}^{new} := \\beta_{0}^{old} - \\alpha  \\frac{1}{N}\\sum_{i=1}^{N} ((\\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 \\cdot X^{(i)}_1 + \\beta_2 \\cdot X^{(i)}_2)}}) - y^{(i)})$$</span>\n",
    "<span></span><br><br>\n",
    "\n",
    "\n",
    "* Al igual que en la Regresión Lineal, actualizamos los parámetros $\\beta_n$ tras recorrer todos los elementos del Dataset; es decir, en cada ***'Epoch'***.\n",
    "\n",
    "\n",
    "* De la misma manera que en la Regresión Lineal, podemos definir una ***condición de parada*** como por ejemplo:\n",
    "\n",
    "    - Haber realizado 'N' Epochs\n",
    "    - El valor de la función de perdida entre una iteración y otra sea el mismo.\n",
    "    - El valor de la función de perdida entre una iteración y otra tengan una diferencia inferior a un umbral definido.\n",
    "    - El valor de los parámetros $\\beta_n$ entre una iteración y otra tengan una diferencia inferior a un umbral definido.\n",
    "    - Etc.\n",
    "    \n",
    "    \n",
    "* El hiperparámetro ***$\\alpha$*** se conoce como ***'Learning Rate'*** y nos indica la cantidad (α) en el que el gradiente descentente tiene que moverse en el sentido contrario a la pendiente de la derivada. En otras palabras, nos ***indicar 'cuanto' se ajusta el modelo o hipótesis en cada uno de los 'Epochs'***.\n",
    "\n",
    "\n",
    "* El hiperparámetro ***$\\alpha$</span> es un valor \"constante\" que debe de asignarse previo a la ejecución***. Por simplicidad en este curso lo consideraremos como constante, pero existen técnicas (como el recocido simulado entre otras) que optimizan el valor de $\\alpha$ para cada epoch.\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "\n",
    "# <a name=\"M4\">4. Pseudocódigo del Gradiente Descendente para la Regresión Logística</a>\n",
    "\n",
    "\n",
    "* Los pasos a dar para obtener el modelo (o hipótesis) para un problema de Clasificación con regresión logística sería el siguiente:\n",
    "\n",
    "    1. Definir valor de $\\alpha$\n",
    "    2. Definir número de Epochs\n",
    "    3. Definir condición de parada\n",
    "    4. Generar modelo (o hipótesis) inicial (de manera aleatoria por ejemplo)\n",
    "    5. Mientras no haya condición de parada ejecutar Epochs:\n",
    "        - Calculamos los errores y el valor de la función de perdida para el modelo o hipótesis 'old'\n",
    "        - Generamos nuevo modelo o hipótesis 'new' actualizando los parámetros $\\beta_n$\n",
    "        \n",
    "        \n",
    "* Pseudocódigo:\n",
    "\n",
    "\n",
    "```python\n",
    "alpha = 0.01\n",
    "num_epochs = 100\n",
    "condicion_parada = False\n",
    "\n",
    "# inicializamos modelo\n",
    "betas_new = random_list(n)\n",
    "beta_new_0 = random()\n",
    "\n",
    "it_counter = 0\n",
    "while not condicion_parada and it_counter < num_epochs:\n",
    "    \n",
    "    # Reasignamos las betas\n",
    "    betas_old = betas_new\n",
    "    beta_old_0 = beta_new_0\n",
    "    \n",
    "    # Inicializamos errores a cero\n",
    "    funcion_perdida = 0\n",
    "    sum_betas = 0\n",
    "    sum_beta_0 = 0\n",
    "    \n",
    "    # Recorremos el Dataset para calcular los errores\n",
    "    for x, y in dataset:\n",
    "        Z = beta_0 + x1 * betas_old_1 + x2 * betas_old_1\n",
    "        hipotesis_i = 1 / (1 + exp(-Z))\n",
    "        error_i = hipotesis_i - y\n",
    "        funcion_perdida += (-y * log(hipotesis_i)) - ((1 - y) * log(1 - hipotesis_i))\n",
    "        for j in betas_old:\n",
    "            sum_betas[j] += error_i * x[j]\n",
    "        sum_beta_0 += error_i\n",
    "    \n",
    "    # Actualizamos los valores de las betas\n",
    "    for j in betas_old:\n",
    "        betas_new[j] = betas_old[j] - (alpha * 1/len(dataset) * sum_betas[j])\n",
    "    beta_new_0 = beta_old_0 - (alpha * 1/len(dataset) * sum_beta_0)\n",
    "    \n",
    "    # Calculamos el valor de la función de perdida del epoch\n",
    "    funcion_perdida = funcion_perdida / len(dataset)\n",
    "    \n",
    "    it_counter+= 1\n",
    "```\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "# <a name=\"M5\">5. Implementación: Regresión Logística</a>\n",
    "\n",
    "\n",
    "* En el siguiente fragmento de código está implementado la ***Regresión Logística con el gradiente descendente***, siguiendo el Pseudocódigo mostrado anteriormente junto con alguna lógica programática adicional.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def print_iteration_status(epoch, betas, beta_0, funcion_perdida):\n",
    "    \"\"\"\n",
    "    Función que imprime por pantalla el resultado del epoch:\n",
    "        - epoch: número del epoch.\n",
    "        - betas: Parámetros de las variables independientes\n",
    "        - beta_0: parámetro 2.\n",
    "        - funcion_perdida: Valor de la función de perdida.\n",
    "    \"\"\"\n",
    "    print('\\nEpoch {}'.format(epoch))\n",
    "    print('\\tZ =  {:0.4f} + {:0.4f} X1 + {:0.4f} X2 '.format(beta_0, betas[0], betas[1]))\n",
    "    print('\\tg(Z) =  1 / 1 + e^-({:0.4f} + {:0.4f} X1 + {:0.4f} X2)'.format(beta_0, betas[0], betas[1]))\n",
    "    print('\\tFunción de Perdida = {:0.4f}'.format(funcion_perdida))\n",
    "\n",
    "\n",
    "def is_convergence(new_betas, old_betas, beta_0_new, beta_0_old, tolerance):\n",
    "    \"\"\"\n",
    "    Función que recibe como argumentos los parámetros antiguos y nuevos de la regresión\n",
    "    y compara los valores para ver si estan por debajo de un umbral y activar la condición\n",
    "    de parada.\n",
    "        - new_params: nuevo valores de los parámetros beta\n",
    "        - old_params: antiguos valores de los parámetros beta\n",
    "        - b_new: nuevo valor del parámetro beta_0\n",
    "        - b_old: antiguo valor del parámetro beta_0\n",
    "        - tolerance: Valor de tolerancia\n",
    "    \"\"\"\n",
    "    convergence = True\n",
    "    for i in range(len(new_betas)):\n",
    "        if math.fabs(new_betas[i] - old_betas[i]) >= tolerance:\n",
    "            convergence = False\n",
    "            break\n",
    "\n",
    "    return convergence and math.fabs(beta_0_new - beta_0_old) < tolerance\n",
    "\n",
    "\n",
    "def logistic_regression(X, y, alpha, num_epochs, tolerance, verbose=False):\n",
    "    \"\"\"\n",
    "    Función que devuelve el modelo o hipótesis de la regresión y los errores por epoch, recibiendo como parámetros:\n",
    "        - X: Valores de la variable independiente.\n",
    "        - y: Valores de la variable dependiente.\n",
    "        - alpha: Learning Rate.\n",
    "        - num_epochs: Número máximo de epochs.\n",
    "        - tolerance: Valor de tolerancia de los parámetros para la condición de parada.\n",
    "        - verbose: Boolean para indicar si queremos imprimir por pantalla el estado del epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Inicializamos los parámetros\n",
    "    random.seed(22)\n",
    "    new_betas = [random.uniform(-5, 5) for i in range(len(X[0]))]\n",
    "    old_betas = [random.uniform(-5, 5) for i in range(len(X[0]))]\n",
    "    beta_0_new, beta_0_old = (random.uniform(-1, 1), random.uniform(-1, 1))\n",
    "\n",
    "    # Contador de Eposchs\n",
    "    it_counter = 0\n",
    "\n",
    "    # Guardamos en una lista el valor de la función de perdida en cada epoch\n",
    "    errores = []\n",
    "\n",
    "    # Epochs\n",
    "    while not is_convergence(new_betas, old_betas, beta_0_new, beta_0_old, tolerance) and it_counter < num_epochs:\n",
    "\n",
    "        # Reasignamos los valores de los parámetros del epoch anterior\n",
    "        old_betas = new_betas\n",
    "        beta_0_old = beta_0_new\n",
    "\n",
    "        # Inicializamos a cero la función de perdida para cada parámetro\n",
    "        funcion_perdida = 0\n",
    "        sum_betas = [0 for i in new_betas]\n",
    "        sum_beta_0 = 0\n",
    "\n",
    "        # Recorremos el Dataset para ver los errores cometidos\n",
    "        for index, elem in enumerate(X):\n",
    "            ecuation = sum(i * j for i, j in zip(old_betas, elem)) + beta_0_old  # Resultado de la ecuación\n",
    "            hipotesis = 1 / (1 + math.exp(-1 * ecuation))                        # Resultado del modelo\n",
    "            error = hipotesis - y[index]                                         # Error cometido en la predicción\n",
    "            funcion_perdida += (y[index] * math.log(hipotesis)) + \\\n",
    "                               ((1 - y[index]) * math.log(1 - hipotesis))        # Acumulamos el valor de la función de perdida\n",
    "            for index2, elem2 in enumerate(sum_betas):                           # Acumulado del error por el valor de X para cada beta\n",
    "                sum_betas[index2] += error * X[index][index2]\n",
    "            sum_beta_0 += error                                                  # Acumulado del error (para cálculo de beta_0)\n",
    "\n",
    "        # Actualizamos Parámetros\n",
    "        for index, elem in enumerate(new_betas):\n",
    "            new_betas[index] = old_betas[index] - ((alpha / len(X)) * sum_betas[index])\n",
    "        beta_0_new = beta_0_old - ((alpha / len(X)) * sum_beta_0)\n",
    "\n",
    "        # Calculamos el error con la función de perdida\n",
    "        funcion_perdida = (-1 * funcion_perdida) / len(X)\n",
    "        errores.append(funcion_perdida)\n",
    "\n",
    "        # Incrementamos contador de Epochs\n",
    "        it_counter += 1\n",
    "\n",
    "        # Imprimimos el estado de la interacción\n",
    "        if verbose:\n",
    "            print_iteration_status(it_counter, new_betas, beta_0_new, funcion_perdida)\n",
    "\n",
    "    return new_betas, beta_0_new, errores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "# <a name=\"M6\">6. Ejemplo de Regresión Logística con Iris</a>\n",
    "\n",
    "* Dado un ***conjunto de datos*** que tiene los ***tamaños del sépalo de dos tipos de flores***, se pretende obtener una expresión que devuelva la ***probabilidad de que una flor*** (conociendo el tamaño del sépalo) pertenezca a cada clase:\n",
    "<span></span><br><br>\n",
    "    <span style=\"font-size:16px\">$P(Y=1|X_1,X_2) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2)}}$</span>\n",
    "<span></span><br><br>\n",
    "    <span style=\"font-size:16px\">$P(Y=0|X_1,X_2) = 1 - \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2)}}$</span>\n",
    "    \n",
    "    \n",
    "* Siendo:\n",
    "    - ***X<sub>1</sub>***: El \"largo\" del Sepalo\n",
    "    - ***X<sub>2</sub>***: El \"ancho\" del Sepalo\n",
    "    \n",
    "\n",
    "\n",
    "* Veamos a continuación el conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sepal_length  sepal_width  petal_length  petal_width            class\n",
       "51           6.4          3.2           4.5          1.5  Iris-versicolor\n",
       "35           5.0          3.2           1.2          0.2      Iris-setosa\n",
       "84           5.4          3.0           4.5          1.5  Iris-versicolor\n",
       "13           4.3          3.0           1.1          0.1      Iris-setosa\n",
       "98           5.1          2.5           3.0          1.1  Iris-versicolor"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/iris/iris_2_categorias.csv\")\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Aunque tenemos más variables, para este ejemplo nos vamos a quedar con:\n",
    "    - ***class***: Clase de la flor (Iris-versicolor, Iris-setosa)\n",
    "    - ***sepal_length***: El \"largo\" del Sepalo\n",
    "    - ***sepal_width***: El \"ancho\" del Sepalo\n",
    "    \n",
    "    \n",
    "* A continuación podemos ver la relación entre las dos variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fe5f346bac8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "sns.swarmplot(x=\"sepal_length\", y=\"sepal_width\", hue=\"class\", data = df, palette=\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Transformación de Datos\n",
    "\n",
    "\n",
    "* Los algoritmos de aprendizaje como la Regresión Logística solo \"entienden de números\" ya que realiza operaciones matemáticas sobre las variables.\n",
    "\n",
    "\n",
    "* En los problemas de clasificación es común que las clases que tengamos en nuestro dataset no sean números si no etiquetas (p.e -> {hombre, mujer}, {mamifero, ave, reptil, pez}, etc).\n",
    "\n",
    "\n",
    "* Por tanto ***tenemos que transformar estas etiquétas en números***. El problema de clasificación que vamos a abordar, tenemos dos clases y le asignaremos los siguientes números:\n",
    "\n",
    "    - ***0***: Iris-Setosa\n",
    "    - ***1***: Iris-Versicolor\n",
    "    \n",
    "    \n",
    "* Para realizar estos tipos de transformaciones, ***Scikit nos proporciona una serie de clases que implementan la interfaz \"Transformers\"*** y que define los métodos ***\"transform()\"*** y ***\"fit_transform()\"*** para realizar las transformaciones necesarias.\n",
    "\n",
    "\n",
    "* Para este caso vamos ha hacer uso de la clase ***LabelEncoder()*** para transformar las etiquetas de las clases.\n",
    "\n",
    "\n",
    "* A continuación procedemos a pasar los datos un array y a realizar las transformaciones:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "X = df[['sepal_length', 'sepal_width']].values   # Obtengo el alto y ancho del sepalo\n",
    "y_labels = df['class'].values                    # Obtengo las etiquetas\n",
    "\n",
    "\n",
    "le = preprocessing.LabelEncoder()     # Instancio un objeto de la clase LabelEncoder()\n",
    "y = le.fit_transform(y_labels)        # Transformo las etiquetas a números"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Ajuste del Modelo\n",
    "\n",
    "* A continuación vamos a ajustar el modelo que nos proporcionará la regresión logística.\n",
    "\n",
    "\n",
    "* Para ello vamos ha hacer uso de la implementación de la regresión logística con el gradiente descendente, pasándole los siguientes parámetros:\n",
    "\n",
    "    - X: Tamaño de las flores\n",
    "    - y: Tipo de flor\n",
    "    - Learning Rate: 0.2\n",
    "    - num_epochs: 50\n",
    "    - tolerancia sobre los parámetros: 0.001\n",
    "    - Verbose: True para ver la evolución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "\tZ =  -0.7315 + 4.0815 X1 + -3.9381 X2 \n",
      "\tg(Z) =  1 / 1 + e^-(-0.7315 + 4.0815 X1 + -3.9381 X2)\n",
      "\tFunción de Perdida = 5.0072\n",
      "\n",
      "Epoch 2\n",
      "\tZ =  -0.8312 + 3.5825 X1 + -4.2787 X2 \n",
      "\tg(Z) =  1 / 1 + e^-(-0.8312 + 3.5825 X1 + -4.2787 X2)\n",
      "\tFunción de Perdida = 3.1218\n",
      "\n",
      "Epoch 3\n",
      "\tZ =  -0.9200 + 3.1380 X1 + -4.5800 X2 \n",
      "\tg(Z) =  1 / 1 + e^-(-0.9200 + 3.1380 X1 + -4.5800 X2)\n",
      "\tFunción de Perdida = 1.3023\n",
      "\n",
      "Epoch 4\n",
      "\tZ =  -0.9518 + 2.9805 X1 + -4.6839 X2 \n",
      "\tg(Z) =  1 / 1 + e^-(-0.9518 + 2.9805 X1 + -4.6839 X2)\n",
      "\tFunción de Perdida = 0.2480\n",
      "\n",
      "Epoch 5\n",
      "\tZ =  -0.9631 + 2.9282 X1 + -4.7208 X2 \n",
      "\tg(Z) =  1 / 1 + e^-(-0.9631 + 2.9282 X1 + -4.7208 X2)\n",
      "\tFunción de Perdida = 0.1277\n",
      "\n",
      "Epoch 6\n",
      "\tZ =  -0.9684 + 2.9076 X1 + -4.7386 X2 \n",
      "\tg(Z) =  1 / 1 + e^-(-0.9684 + 2.9076 X1 + -4.7386 X2)\n",
      "\tFunción de Perdida = 0.1127\n",
      "\n",
      "Epoch 7\n",
      "\tZ =  -0.9712 + 2.9002 X1 + -4.7487 X2 \n",
      "\tg(Z) =  1 / 1 + e^-(-0.9712 + 2.9002 X1 + -4.7487 X2)\n",
      "\tFunción de Perdida = 0.1099\n",
      "\n",
      "Epoch 8\n",
      "\tZ =  -0.9729 + 2.8985 X1 + -4.7555 X2 \n",
      "\tg(Z) =  1 / 1 + e^-(-0.9729 + 2.8985 X1 + -4.7555 X2)\n",
      "\tFunción de Perdida = 0.1093\n",
      "\n",
      "Epoch 9\n",
      "\tZ =  -0.9742 + 2.8992 X1 + -4.7609 X2 \n",
      "\tg(Z) =  1 / 1 + e^-(-0.9742 + 2.8992 X1 + -4.7609 X2)\n",
      "\tFunción de Perdida = 0.1091\n",
      "\n",
      "Epoch 10\n",
      "\tZ =  -0.9752 + 2.9011 X1 + -4.7657 X2 \n",
      "\tg(Z) =  1 / 1 + e^-(-0.9752 + 2.9011 X1 + -4.7657 X2)\n",
      "\tFunción de Perdida = 0.1089\n",
      "\n",
      "Epoch 11\n",
      "\tZ =  -0.9762 + 2.9033 X1 + -4.7701 X2 \n",
      "\tg(Z) =  1 / 1 + e^-(-0.9762 + 2.9033 X1 + -4.7701 X2)\n",
      "\tFunción de Perdida = 0.1088\n"
     ]
    }
   ],
   "source": [
    "betas, beta_0, funcion_perdida = logistic_regression(X=X, y=y, alpha=0.2, num_epochs=50, tolerance=0.001, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Evolución de la función del perdida por epoch\n",
    "\n",
    "\n",
    "* Podemos observar tambien como evoluciona la función de perdida por epoch, viendo como en cada epoch el error es menor que en el epoch anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax =  plt.subplots(figsize=(6, 4))\n",
    "ax.plot([index for index, value in enumerate(funcion_perdida)], [error for error in funcion_perdida], color='green')\n",
    "ax.set_title(\"Función de perdida por Epoch\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Función de Perdida\")\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Acierto | Error del modelo generado\n",
    "\n",
    "\n",
    "* El Dataset con el que estamos trabajando tiene 100 elementos y podemos ver (aplicando el modelo obtenido por la regresión logística) como de bien o de mal clasifica estas flores:\n",
    "\n",
    "\n",
    "* Mostramos a continuación para cada elemento del Dataset, cual es su predicción y si acierta o falla el tipo de flor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Elemento 1\n",
      "  Z =  -0.98 + 2.90·5.10 + -4.77·5.10 = -2.86\n",
      "  g(Z) =  1 / 1 + e^-(-2.86) = 0.05\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 2\n",
      "  Z =  -0.98 + 2.90·4.90 + -4.77·4.90 = -1.06\n",
      "  g(Z) =  1 / 1 + e^-(-1.06) = 0.26\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 3\n",
      "  Z =  -0.98 + 2.90·4.70 + -4.77·4.70 = -2.60\n",
      "  g(Z) =  1 / 1 + e^-(-2.60) = 0.07\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 4\n",
      "  Z =  -0.98 + 2.90·4.60 + -4.77·4.60 = -2.41\n",
      "  g(Z) =  1 / 1 + e^-(-2.41) = 0.08\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 5\n",
      "  Z =  -0.98 + 2.90·5.00 + -4.77·5.00 = -3.63\n",
      "  g(Z) =  1 / 1 + e^-(-3.63) = 0.03\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 6\n",
      "  Z =  -0.98 + 2.90·5.40 + -4.77·5.40 = -3.90\n",
      "  g(Z) =  1 / 1 + e^-(-3.90) = 0.02\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 7\n",
      "  Z =  -0.98 + 2.90·4.60 + -4.77·4.60 = -3.84\n",
      "  g(Z) =  1 / 1 + e^-(-3.84) = 0.02\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 8\n",
      "  Z =  -0.98 + 2.90·5.00 + -4.77·5.00 = -2.68\n",
      "  g(Z) =  1 / 1 + e^-(-2.68) = 0.06\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 9\n",
      "  Z =  -0.98 + 2.90·4.40 + -4.77·4.40 = -2.04\n",
      "  g(Z) =  1 / 1 + e^-(-2.04) = 0.12\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 10\n",
      "  Z =  -0.98 + 2.90·4.90 + -4.77·4.90 = -1.54\n",
      "  g(Z) =  1 / 1 + e^-(-1.54) = 0.18\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 11\n",
      "  Z =  -0.98 + 2.90·5.40 + -4.77·5.40 = -2.95\n",
      "  g(Z) =  1 / 1 + e^-(-2.95) = 0.05\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 12\n",
      "  Z =  -0.98 + 2.90·4.80 + -4.77·4.80 = -3.26\n",
      "  g(Z) =  1 / 1 + e^-(-3.26) = 0.04\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 13\n",
      "  Z =  -0.98 + 2.90·4.80 + -4.77·4.80 = -1.35\n",
      "  g(Z) =  1 / 1 + e^-(-1.35) = 0.21\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 14\n",
      "  Z =  -0.98 + 2.90·4.30 + -4.77·4.30 = -2.80\n",
      "  g(Z) =  1 / 1 + e^-(-2.80) = 0.06\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 15\n",
      "  Z =  -0.98 + 2.90·5.80 + -4.77·5.80 = -3.22\n",
      "  g(Z) =  1 / 1 + e^-(-3.22) = 0.04\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 16\n",
      "  Z =  -0.98 + 2.90·5.70 + -4.77·5.70 = -5.42\n",
      "  g(Z) =  1 / 1 + e^-(-5.42) = 0.00\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 17\n",
      "  Z =  -0.98 + 2.90·5.40 + -4.77·5.40 = -3.90\n",
      "  g(Z) =  1 / 1 + e^-(-3.90) = 0.02\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 18\n",
      "  Z =  -0.98 + 2.90·5.10 + -4.77·5.10 = -2.86\n",
      "  g(Z) =  1 / 1 + e^-(-2.86) = 0.05\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 19\n",
      "  Z =  -0.98 + 2.90·5.70 + -4.77·5.70 = -2.55\n",
      "  g(Z) =  1 / 1 + e^-(-2.55) = 0.07\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 20\n",
      "  Z =  -0.98 + 2.90·5.10 + -4.77·5.10 = -4.30\n",
      "  g(Z) =  1 / 1 + e^-(-4.30) = 0.01\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 21\n",
      "  Z =  -0.98 + 2.90·5.40 + -4.77·5.40 = -1.52\n",
      "  g(Z) =  1 / 1 + e^-(-1.52) = 0.18\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 22\n",
      "  Z =  -0.98 + 2.90·5.10 + -4.77·5.10 = -3.82\n",
      "  g(Z) =  1 / 1 + e^-(-3.82) = 0.02\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 23\n",
      "  Z =  -0.98 + 2.90·4.60 + -4.77·4.60 = -4.79\n",
      "  g(Z) =  1 / 1 + e^-(-4.79) = 0.01\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 24\n",
      "  Z =  -0.98 + 2.90·5.10 + -4.77·5.10 = -1.91\n",
      "  g(Z) =  1 / 1 + e^-(-1.91) = 0.13\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 25\n",
      "  Z =  -0.98 + 2.90·4.80 + -4.77·4.80 = -3.26\n",
      "  g(Z) =  1 / 1 + e^-(-3.26) = 0.04\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 26\n",
      "  Z =  -0.98 + 2.90·5.00 + -4.77·5.00 = -0.77\n",
      "  g(Z) =  1 / 1 + e^-(-0.77) = 0.32\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 27\n",
      "  Z =  -0.98 + 2.90·5.00 + -4.77·5.00 = -2.68\n",
      "  g(Z) =  1 / 1 + e^-(-2.68) = 0.06\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 28\n",
      "  Z =  -0.98 + 2.90·5.20 + -4.77·5.20 = -2.57\n",
      "  g(Z) =  1 / 1 + e^-(-2.57) = 0.07\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 29\n",
      "  Z =  -0.98 + 2.90·5.20 + -4.77·5.20 = -2.10\n",
      "  g(Z) =  1 / 1 + e^-(-2.10) = 0.11\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 30\n",
      "  Z =  -0.98 + 2.90·4.70 + -4.77·4.70 = -2.60\n",
      "  g(Z) =  1 / 1 + e^-(-2.60) = 0.07\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 31\n",
      "  Z =  -0.98 + 2.90·4.80 + -4.77·4.80 = -1.83\n",
      "  g(Z) =  1 / 1 + e^-(-1.83) = 0.14\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 32\n",
      "  Z =  -0.98 + 2.90·5.40 + -4.77·5.40 = -1.52\n",
      "  g(Z) =  1 / 1 + e^-(-1.52) = 0.18\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 33\n",
      "  Z =  -0.98 + 2.90·5.20 + -4.77·5.20 = -5.44\n",
      "  g(Z) =  1 / 1 + e^-(-5.44) = 0.00\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 34\n",
      "  Z =  -0.98 + 2.90·5.50 + -4.77·5.50 = -5.04\n",
      "  g(Z) =  1 / 1 + e^-(-5.04) = 0.01\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 35\n",
      "  Z =  -0.98 + 2.90·4.90 + -4.77·4.90 = -1.54\n",
      "  g(Z) =  1 / 1 + e^-(-1.54) = 0.18\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 36\n",
      "  Z =  -0.98 + 2.90·5.00 + -4.77·5.00 = -1.72\n",
      "  g(Z) =  1 / 1 + e^-(-1.72) = 0.15\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 37\n",
      "  Z =  -0.98 + 2.90·5.50 + -4.77·5.50 = -1.70\n",
      "  g(Z) =  1 / 1 + e^-(-1.70) = 0.15\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 38\n",
      "  Z =  -0.98 + 2.90·4.90 + -4.77·4.90 = -1.54\n",
      "  g(Z) =  1 / 1 + e^-(-1.54) = 0.18\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 39\n",
      "  Z =  -0.98 + 2.90·4.40 + -4.77·4.40 = -2.51\n",
      "  g(Z) =  1 / 1 + e^-(-2.51) = 0.08\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 40\n",
      "  Z =  -0.98 + 2.90·5.10 + -4.77·5.10 = -2.39\n",
      "  g(Z) =  1 / 1 + e^-(-2.39) = 0.08\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 41\n",
      "  Z =  -0.98 + 2.90·5.00 + -4.77·5.00 = -3.16\n",
      "  g(Z) =  1 / 1 + e^-(-3.16) = 0.04\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 42\n",
      "  Z =  -0.98 + 2.90·4.50 + -4.77·4.50 = 1.12\n",
      "  g(Z) =  1 / 1 + e^-(1.12) = 0.75\n",
      "Y_real = 0 - Y_predic = 1 - Acieto = False\n",
      "\n",
      "Elemento 43\n",
      "  Z =  -0.98 + 2.90·4.40 + -4.77·4.40 = -3.47\n",
      "  g(Z) =  1 / 1 + e^-(-3.47) = 0.03\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 44\n",
      "  Z =  -0.98 + 2.90·5.00 + -4.77·5.00 = -3.16\n",
      "  g(Z) =  1 / 1 + e^-(-3.16) = 0.04\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 45\n",
      "  Z =  -0.98 + 2.90·5.10 + -4.77·5.10 = -4.30\n",
      "  g(Z) =  1 / 1 + e^-(-4.30) = 0.01\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 46\n",
      "  Z =  -0.98 + 2.90·4.80 + -4.77·4.80 = -1.35\n",
      "  g(Z) =  1 / 1 + e^-(-1.35) = 0.21\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 47\n",
      "  Z =  -0.98 + 2.90·5.10 + -4.77·5.10 = -4.30\n",
      "  g(Z) =  1 / 1 + e^-(-4.30) = 0.01\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 48\n",
      "  Z =  -0.98 + 2.90·4.60 + -4.77·4.60 = -2.89\n",
      "  g(Z) =  1 / 1 + e^-(-2.89) = 0.05\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 49\n",
      "  Z =  -0.98 + 2.90·5.30 + -4.77·5.30 = -3.24\n",
      "  g(Z) =  1 / 1 + e^-(-3.24) = 0.04\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 50\n",
      "  Z =  -0.98 + 2.90·5.00 + -4.77·5.00 = -2.20\n",
      "  g(Z) =  1 / 1 + e^-(-2.20) = 0.10\n",
      "Y_real = 0 - Y_predic = 0 - Acieto = True\n",
      "\n",
      "Elemento 51\n",
      "  Z =  -0.98 + 2.90·7.00 + -4.77·7.00 = 4.08\n",
      "  g(Z) =  1 / 1 + e^-(4.08) = 0.98\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 52\n",
      "  Z =  -0.98 + 2.90·6.40 + -4.77·6.40 = 2.34\n",
      "  g(Z) =  1 / 1 + e^-(2.34) = 0.91\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 53\n",
      "  Z =  -0.98 + 2.90·6.90 + -4.77·6.90 = 4.27\n",
      "  g(Z) =  1 / 1 + e^-(4.27) = 0.99\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 54\n",
      "  Z =  -0.98 + 2.90·5.50 + -4.77·5.50 = 4.02\n",
      "  g(Z) =  1 / 1 + e^-(4.02) = 0.98\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 55\n",
      "  Z =  -0.98 + 2.90·6.50 + -4.77·6.50 = 4.54\n",
      "  g(Z) =  1 / 1 + e^-(4.54) = 0.99\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 56\n",
      "  Z =  -0.98 + 2.90·5.70 + -4.77·5.70 = 2.22\n",
      "  g(Z) =  1 / 1 + e^-(2.22) = 0.90\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 57\n",
      "  Z =  -0.98 + 2.90·6.30 + -4.77·6.30 = 1.57\n",
      "  g(Z) =  1 / 1 + e^-(1.57) = 0.83\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 58\n",
      "  Z =  -0.98 + 2.90·4.90 + -4.77·4.90 = 1.80\n",
      "  g(Z) =  1 / 1 + e^-(1.80) = 0.86\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 59\n",
      "  Z =  -0.98 + 2.90·6.60 + -4.77·6.60 = 4.35\n",
      "  g(Z) =  1 / 1 + e^-(4.35) = 0.99\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 60\n",
      "  Z =  -0.98 + 2.90·5.20 + -4.77·5.20 = 1.24\n",
      "  g(Z) =  1 / 1 + e^-(1.24) = 0.78\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 61\n",
      "  Z =  -0.98 + 2.90·5.00 + -4.77·5.00 = 4.00\n",
      "  g(Z) =  1 / 1 + e^-(4.00) = 0.98\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 62\n",
      "  Z =  -0.98 + 2.90·5.90 + -4.77·5.90 = 1.84\n",
      "  g(Z) =  1 / 1 + e^-(1.84) = 0.86\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 63\n",
      "  Z =  -0.98 + 2.90·6.00 + -4.77·6.00 = 5.95\n",
      "  g(Z) =  1 / 1 + e^-(5.95) = 1.00\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 64\n",
      "  Z =  -0.98 + 2.90·6.10 + -4.77·6.10 = 2.90\n",
      "  g(Z) =  1 / 1 + e^-(2.90) = 0.95\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 65\n",
      "  Z =  -0.98 + 2.90·5.60 + -4.77·5.60 = 1.45\n",
      "  g(Z) =  1 / 1 + e^-(1.45) = 0.81\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 66\n",
      "  Z =  -0.98 + 2.90·6.70 + -4.77·6.70 = 3.69\n",
      "  g(Z) =  1 / 1 + e^-(3.69) = 0.98\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 67\n",
      "  Z =  -0.98 + 2.90·5.60 + -4.77·5.60 = 0.97\n",
      "  g(Z) =  1 / 1 + e^-(0.97) = 0.73\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 68\n",
      "  Z =  -0.98 + 2.90·5.80 + -4.77·5.80 = 2.98\n",
      "  g(Z) =  1 / 1 + e^-(2.98) = 0.95\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 69\n",
      "  Z =  -0.98 + 2.90·6.20 + -4.77·6.20 = 6.53\n",
      "  g(Z) =  1 / 1 + e^-(6.53) = 1.00\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 70\n",
      "  Z =  -0.98 + 2.90·5.60 + -4.77·5.60 = 3.36\n",
      "  g(Z) =  1 / 1 + e^-(3.36) = 0.97\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 71\n",
      "  Z =  -0.98 + 2.90·5.90 + -4.77·5.90 = 0.89\n",
      "  g(Z) =  1 / 1 + e^-(0.89) = 0.71\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 72\n",
      "  Z =  -0.98 + 2.90·6.10 + -4.77·6.10 = 3.38\n",
      "  g(Z) =  1 / 1 + e^-(3.38) = 0.97\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 73\n",
      "  Z =  -0.98 + 2.90·6.30 + -4.77·6.30 = 5.39\n",
      "  g(Z) =  1 / 1 + e^-(5.39) = 1.00\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 74\n",
      "  Z =  -0.98 + 2.90·6.10 + -4.77·6.10 = 3.38\n",
      "  g(Z) =  1 / 1 + e^-(3.38) = 0.97\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 75\n",
      "  Z =  -0.98 + 2.90·6.40 + -4.77·6.40 = 3.77\n",
      "  g(Z) =  1 / 1 + e^-(3.77) = 0.98\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 76\n",
      "  Z =  -0.98 + 2.90·6.60 + -4.77·6.60 = 3.88\n",
      "  g(Z) =  1 / 1 + e^-(3.88) = 0.98\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 77\n",
      "  Z =  -0.98 + 2.90·6.80 + -4.77·6.80 = 5.41\n",
      "  g(Z) =  1 / 1 + e^-(5.41) = 1.00\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 78\n",
      "  Z =  -0.98 + 2.90·6.70 + -4.77·6.70 = 4.17\n",
      "  g(Z) =  1 / 1 + e^-(4.17) = 0.98\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 79\n",
      "  Z =  -0.98 + 2.90·6.00 + -4.77·6.00 = 2.61\n",
      "  g(Z) =  1 / 1 + e^-(2.61) = 0.93\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 80\n",
      "  Z =  -0.98 + 2.90·5.70 + -4.77·5.70 = 3.17\n",
      "  g(Z) =  1 / 1 + e^-(3.17) = 0.96\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 81\n",
      "  Z =  -0.98 + 2.90·5.50 + -4.77·5.50 = 3.54\n",
      "  g(Z) =  1 / 1 + e^-(3.54) = 0.97\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 82\n",
      "  Z =  -0.98 + 2.90·5.50 + -4.77·5.50 = 3.54\n",
      "  g(Z) =  1 / 1 + e^-(3.54) = 0.97\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 83\n",
      "  Z =  -0.98 + 2.90·5.80 + -4.77·5.80 = 2.98\n",
      "  g(Z) =  1 / 1 + e^-(2.98) = 0.95\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 84\n",
      "  Z =  -0.98 + 2.90·6.00 + -4.77·6.00 = 3.56\n",
      "  g(Z) =  1 / 1 + e^-(3.56) = 0.97\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 85\n",
      "  Z =  -0.98 + 2.90·5.40 + -4.77·5.40 = 0.39\n",
      "  g(Z) =  1 / 1 + e^-(0.39) = 0.60\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 86\n",
      "  Z =  -0.98 + 2.90·6.00 + -4.77·6.00 = 0.23\n",
      "  g(Z) =  1 / 1 + e^-(0.23) = 0.56\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 87\n",
      "  Z =  -0.98 + 2.90·6.70 + -4.77·6.70 = 3.69\n",
      "  g(Z) =  1 / 1 + e^-(3.69) = 0.98\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 88\n",
      "  Z =  -0.98 + 2.90·6.30 + -4.77·6.30 = 6.34\n",
      "  g(Z) =  1 / 1 + e^-(6.34) = 1.00\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 89\n",
      "  Z =  -0.98 + 2.90·5.60 + -4.77·5.60 = 0.97\n",
      "  g(Z) =  1 / 1 + e^-(0.97) = 0.73\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 90\n",
      "  Z =  -0.98 + 2.90·5.50 + -4.77·5.50 = 3.07\n",
      "  g(Z) =  1 / 1 + e^-(3.07) = 0.96\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 91\n",
      "  Z =  -0.98 + 2.90·5.50 + -4.77·5.50 = 2.59\n",
      "  g(Z) =  1 / 1 + e^-(2.59) = 0.93\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 92\n",
      "  Z =  -0.98 + 2.90·6.10 + -4.77·6.10 = 2.42\n",
      "  g(Z) =  1 / 1 + e^-(2.42) = 0.92\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 93\n",
      "  Z =  -0.98 + 2.90·5.80 + -4.77·5.80 = 3.46\n",
      "  g(Z) =  1 / 1 + e^-(3.46) = 0.97\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 94\n",
      "  Z =  -0.98 + 2.90·5.00 + -4.77·5.00 = 2.57\n",
      "  g(Z) =  1 / 1 + e^-(2.57) = 0.93\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 95\n",
      "  Z =  -0.98 + 2.90·5.60 + -4.77·5.60 = 2.40\n",
      "  g(Z) =  1 / 1 + e^-(2.40) = 0.92\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 96\n",
      "  Z =  -0.98 + 2.90·5.70 + -4.77·5.70 = 1.26\n",
      "  g(Z) =  1 / 1 + e^-(1.26) = 0.78\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 97\n",
      "  Z =  -0.98 + 2.90·5.70 + -4.77·5.70 = 1.74\n",
      "  g(Z) =  1 / 1 + e^-(1.74) = 0.85\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 98\n",
      "  Z =  -0.98 + 2.90·6.20 + -4.77·6.20 = 3.19\n",
      "  g(Z) =  1 / 1 + e^-(3.19) = 0.96\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 99\n",
      "  Z =  -0.98 + 2.90·5.10 + -4.77·5.10 = 1.91\n",
      "  g(Z) =  1 / 1 + e^-(1.91) = 0.87\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Elemento 100\n",
      "  Z =  -0.98 + 2.90·5.70 + -4.77·5.70 = 2.22\n",
      "  g(Z) =  1 / 1 + e^-(2.22) = 0.90\n",
      "Y_real = 1 - Y_predic = 1 - Acieto = True\n",
      "\n",
      "Acierto = 99.0 %\n",
      "Error = 1.0 %\n"
     ]
    }
   ],
   "source": [
    "error = 0\n",
    "for index, elem in enumerate(X):\n",
    "    ecuation = sum(i * j for i, j in zip(betas, elem)) + beta_0\n",
    "    hipotesis = round(1 / (1 + math.exp(-1 * ecuation)))\n",
    "    error += abs(hipotesis - y[index])\n",
    "    \n",
    "    # Imprimimos por pantalla la información de cada elemento\n",
    "    print('\\nElemento {}'.format(index + 1))\n",
    "    print('  Z =  {:0.2f} + {:0.2f}·{:0.2f} + {:0.2f}·{:0.2f} = {:0.2f}'.\n",
    "          format(beta_0, betas[0], X[index][0], betas[1], X[index][0], ecuation))\n",
    "    print('  g(Z) =  1 / 1 + e^-({:0.2f}) = {:0.2f}'.format(ecuation, 1 / (1 + math.exp(-1 * ecuation))))\n",
    "    print('Y_real = {} - Y_predic = {} - Acieto = {}'.format(y[index], hipotesis, y[index]==hipotesis))\n",
    "    \n",
    "error = error / len(X)\n",
    "    \n",
    "print('\\nAcierto = {} %'.format((1-error) * 100))\n",
    "print('Error = {} %'.format(error * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "*Este Notebook ha sido desarrollado por **Ricardo Moya García** y registrado en Safe Creative como ***Atribución-NoComercial-CompartirIgual***.*\n",
    "\n",
    "<img src=\"./imgs/CC_BY-NC-SA.png\" alt=\"CC BY-NC\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
